{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aa42bf1-62d0-4775-9698-20c97892e762",
   "metadata": {},
   "source": [
    "# Linear Regression  \n",
    "> #### Moussa JAMOR :\n",
    "> [https://github.com/JamorMoussa](https://github.com/JamorMoussa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8adabb-abf6-42f7-a636-41f5c895395d",
   "metadata": {},
   "source": [
    "## Contents:\n",
    "* ### [Theory : Simple Linear Regression](#simplelg)\n",
    "    * #### [Linear regression formula](#lgf)\n",
    "    * #### [Least Mean Squares](#lms)\n",
    "    * #### [Gradient Descent MSE](#mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30665e2-a323-4382-81ff-83956d2599fc",
   "metadata": {},
   "source": [
    "## Simple Linear Regression : <a id=\"simplelg\"> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4825f9b-55cb-430d-88c9-c8699ab93df0",
   "metadata": {},
   "source": [
    "**Linear Regression** is a classic machine learning algorithm, that predict the linear relationship between an independent variable $x$ and a dependent variable $y$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb643c43-9bb6-4b3f-98db-c30f284cc979",
   "metadata": {},
   "source": [
    "Let the training data $S = \\left \\{(x_i, y_i) \\right \\}_{i = 0}^{m}$, where $x_i \\in \\mathbb{R}^d$ and $y \\in \\mathbb{R}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936fd69a-ba0e-4b14-b866-e8e15f8de2ed",
   "metadata": {},
   "source": [
    "### Linear regression formula : <a id = \"lgf\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0463d0-3207-492b-aca3-22f7d04c421c",
   "metadata": {},
   "source": [
    "The purpose is find a regressor $h_s$ such as : \n",
    "\n",
    "$$ h_s(x) = w^T.x + w_0 \\Longleftrightarrow  h_s(x) = w^T.x $$\n",
    "\n",
    "where $w \\in \\mathbb{R}^{d+1} $ by including the bias $w_0$, and x = $\\begin{bmatrix}\n",
    "                                                                            1 \\\\\n",
    "                                                                           x_1 \\\\\n",
    "                                                                           \\vdots \\\\\n",
    "                                                                           x_d \\\\\n",
    "                                                                          \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf20ff2-99e5-47da-9b34-b0408c303549",
   "metadata": {},
   "source": [
    "The vector $w$ is inistized randomly, the purpose is make $h_s$ close to $y$ as much as possible, at least for the training examples $S$ we have. To formalize this, we define a function that measure how close $h_s(x_i)$ to label $y_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1881cd-d11d-46ba-aaae-bef54d9ee1f1",
   "metadata": {},
   "source": [
    "### Mean squared error: <a id = \"mse\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc09e350-2097-4f69-9dfb-881f83935334",
   "metadata": {},
   "source": [
    "We define the **Mean squared error** as our **Cost function :** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256a076f-a633-447f-9abb-f2cb86719992",
   "metadata": {},
   "source": [
    "$$ J(w) = \\frac{1}{m} \\sum_{i=1}^{m} (h_s(x_i) - y_i)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1820d3b2-bb38-43c8-b889-81cb0c6a7e6b",
   "metadata": {},
   "source": [
    "We aim to select the parameter vector $w$ to minimize the cost function $J(w)$. To achieve this, we employ a search algorithm that commences with an initial guess $w_0$ and iteratively updates the model parameters $w$ until the cost function $J(w)$ is minimized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f1868e-ee0f-4e5d-ad66-45e6b9cf89ac",
   "metadata": {},
   "source": [
    "### Gradient Descent MSE  : <a id=\"lms\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad35545-f670-4f25-af28-631531df4659",
   "metadata": {},
   "source": [
    "Specialy, let consider a [Gradient descent algorithm](https://en.wikipedia.org/wiki/Gradient_descent), which start with $w_0$ and peatedly performs the update : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9091e7f5-00b9-48f5-ad79-6885ddf384ab",
   "metadata": {},
   "source": [
    "$$ w_{j+1} = w_{j} - \\alpha . \\frac{\\partial J(w_j)}{\\partial w_j}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55719975-3db9-4197-ae0d-9186e71abfa8",
   "metadata": {},
   "source": [
    "where $\\alpha \\in \\mathbb{R}^+$ is an hyperparamters for the model, called **learning rate**. In order to impliment this algorithm let find the partial derivative of $J(w_i)$ repect to $w_i$ :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ebeb6f-9609-4650-b4bd-f4c1170aec2d",
   "metadata": {},
   "source": [
    " $ \\hspace{3cm} \\text{ for } j \\in \\{0, ..., d+1\\}$ : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6645aa95-25b4-4944-a67e-baf3e9a00909",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial J(w_j)}{\\partial w_j} = \\frac{\\partial}{\\partial w_j} \\left( \\frac{1}{m} \\sum_{i=1}^{m} (h_s(x_i) - y_i)^2  \\right) $$\n",
    "\n",
    "$$= \\frac{1}{m} \\sum_{i=1}^{m} \\frac{\\partial}{\\partial w_j} \\left(h_s(x_i) - y_i\\right)^2 $$\n",
    "\n",
    "$$= \\frac{2}{m} \\sum_{i=1}^{m} \\left(w_j^T.x_i - y_i\\right).x_j $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39a21c3-6d6d-4545-abce-ac57102a1ae1",
   "metadata": {},
   "source": [
    "Finaly, \n",
    "\n",
    "$$ w_{j+1} = w_{j} - \\alpha. \\frac{2}{m} \\sum_{i=1}^{m} \\left(w_j^T.x_i - y_i\\right).x_j \\mid  \\text{ for } j \\in \\{0, ..., d+1\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783769b5-76b2-4ca8-b2fd-11b803f41cbf",
   "metadata": {},
   "source": [
    "By grouping the updates of the coordinates into an update of the vector $w$ : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e972d8-8768-43d0-9826-a3197ebde52f",
   "metadata": {},
   "source": [
    "$$ w = w - \\alpha. \\frac{2}{m} \\sum_{i=1}^{m} \\left(h_s(x_i) - y_i\\right).x_i $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e91bf5-4764-4753-af03-a0ea83504c86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4ce7a8-edb1-4dea-a6bf-266acd22178a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6793fa0f-50ec-4b40-92e7-d3f4d400ef5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
