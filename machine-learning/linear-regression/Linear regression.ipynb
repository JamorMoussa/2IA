{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aa42bf1-62d0-4775-9698-20c97892e762",
   "metadata": {},
   "source": [
    "# Linear Regression  \n",
    "> #### Moussa JAMOR :\n",
    "> [https://github.com/JamorMoussa](https://github.com/JamorMoussa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8adabb-abf6-42f7-a636-41f5c895395d",
   "metadata": {},
   "source": [
    "## Contents:\n",
    "* ### [Theory : Simple Linear Regression](#)\n",
    "    * #### [Linear regression formula](#)\n",
    "    * #### [Least Mean Squares](#)\n",
    "    * #### [Gradient Descent MSE](#)\n",
    "    * ### [Matrix notation](#) \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30665e2-a323-4382-81ff-83956d2599fc",
   "metadata": {},
   "source": [
    "## Simple Linear Regression :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4825f9b-55cb-430d-88c9-c8699ab93df0",
   "metadata": {},
   "source": [
    "**Linear Regression** is a classic machine learning algorithm, that predict the linear relationship between an independent variable $x$ and a dependent variable $y$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb643c43-9bb6-4b3f-98db-c30f284cc979",
   "metadata": {},
   "source": [
    "Let the training data $S = \\left \\{(x_i, y_i) \\right \\}_{i = 0}^{m}$, where $x_i \\in \\mathbb{R}^d$ and $y \\in \\mathbb{R}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936fd69a-ba0e-4b14-b866-e8e15f8de2ed",
   "metadata": {},
   "source": [
    "### Linear regression formula : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0463d0-3207-492b-aca3-22f7d04c421c",
   "metadata": {},
   "source": [
    "The purpose is find a regressor $h_s$ such as : \n",
    "\n",
    "$$ h_s(x) = w^T.x + w_0 \\Longleftrightarrow  h_s(x) = w^T.x $$\n",
    "\n",
    "where $w \\in \\mathbb{R}^{d+1} $ by including the bias $w_0$, and x = $\\begin{bmatrix}\n",
    "                                                                            1 \\\\\n",
    "                                                                           x_1 \\\\\n",
    "                                                                           \\vdots \\\\\n",
    "                                                                           x_d \\\\\n",
    "                                                                          \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf20ff2-99e5-47da-9b34-b0408c303549",
   "metadata": {},
   "source": [
    "The vector $w$ is inistized randomly, the purpose is make $h_s$ close to $y$ as much as possible, at least for the training examples $S$ we have. To formalize this, we define a function that measure how close $h_s(x_i)$ to label $y_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1881cd-d11d-46ba-aaae-bef54d9ee1f1",
   "metadata": {},
   "source": [
    "### Mean squared error: <a id = \"mse\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc09e350-2097-4f69-9dfb-881f83935334",
   "metadata": {},
   "source": [
    "We define the **Mean squared error** as our **Cost function :** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256a076f-a633-447f-9abb-f2cb86719992",
   "metadata": {},
   "source": [
    "$$ J(w) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_s(x_i) - y_i)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1820d3b2-bb38-43c8-b889-81cb0c6a7e6b",
   "metadata": {},
   "source": [
    "We aim to select the parameter vector $w$ to minimize the cost function $J(w)$. To achieve this, we employ a search algorithm that commences with an initial guess $w_0$ and iteratively updates the model parameters $w$ until the cost function $J(w)$ is minimized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f1868e-ee0f-4e5d-ad66-45e6b9cf89ac",
   "metadata": {},
   "source": [
    "### Gradient Descent MSE  : <a id=\"lms\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad35545-f670-4f25-af28-631531df4659",
   "metadata": {},
   "source": [
    "Specialy, let consider a [Gradient descent algorithm](https://en.wikipedia.org/wiki/Gradient_descent), which start with $w_0$ and peatedly performs the update : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9091e7f5-00b9-48f5-ad79-6885ddf384ab",
   "metadata": {},
   "source": [
    "$$ w_{j+1} = w_{j} - \\alpha . \\frac{\\partial J(w_j)}{\\partial w_j}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55719975-3db9-4197-ae0d-9186e71abfa8",
   "metadata": {},
   "source": [
    "where $\\alpha \\in \\mathbb{R}^+$ is an hyperparamters for the model, called **learning rate**. In order to impliment this algorithm let find the partial derivative of $J(w_i)$ repect to $w_i$ :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ebeb6f-9609-4650-b4bd-f4c1170aec2d",
   "metadata": {},
   "source": [
    " $ \\hspace{3cm} \\text{ for } j \\in \\{0, ..., d+1\\}$ : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6645aa95-25b4-4944-a67e-baf3e9a00909",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial J(w_j)}{\\partial w_j} = \\frac{\\partial}{\\partial w_j} \\left( \\frac{1}{2m} \\sum_{i=1}^{m} (h_s(x_i) - y_i)^2  \\right) $$\n",
    "\n",
    "$$= \\frac{1}{2m} \\sum_{i=1}^{m} \\frac{\\partial}{\\partial w_j} \\left(h_s(x_i) - y_i\\right)^2 $$\n",
    "\n",
    "$$= \\frac{1}{m} \\sum_{i=1}^{m} \\left(w_j^T.x_i - y_i\\right).x_j $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39a21c3-6d6d-4545-abce-ac57102a1ae1",
   "metadata": {},
   "source": [
    "finally, \n",
    "\n",
    "$$ w_{j+1} = w_{j} - \\alpha. \\frac{1}{m} \\sum_{i=1}^{m} \\left(w_j^T.x_i - y_i\\right).x_j \\mid  \\text{ for } j \\in \\{0, ..., d+1\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783769b5-76b2-4ca8-b2fd-11b803f41cbf",
   "metadata": {},
   "source": [
    "By grouping the updates of the coordinates into an update of the vector $w$ : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e972d8-8768-43d0-9826-a3197ebde52f",
   "metadata": {},
   "source": [
    "$$ w = w - \\alpha. \\frac{1}{m} \\sum_{i=1}^{m} \\left(h_s(x_i) - y_i\\right).x_i $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f417b930-d4f6-49ed-832d-49d1d6370e8e",
   "metadata": {},
   "source": [
    "### Matrix notation : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4cc7d7-5461-4adc-a370-3dbdb7c890f1",
   "metadata": {},
   "source": [
    "Let's discuss an other way to solve the same problem without using the gradient descent. In this time we will use the matrix notation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3091cc-d6b3-4519-b757-9974e3b11b2b",
   "metadata": {},
   "source": [
    "Let's consider that : \n",
    "\n",
    "$$X = \\begin{bmatrix}\n",
    " \\textemdash & x_0^T & \\textemdash \\\\\n",
    " &  \\vdots  & \\\\\n",
    " \\textemdash & x_i^T & \\textemdash \\\\\n",
    " &  \\vdots  & \\\\\n",
    " \\textemdash & x_m^T & \\textemdash \\\\\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{m\\times d} \\hspace{0.5cm} \\text{ and } \\hspace{0.5cm} y = \\begin{bmatrix}\n",
    " & y_0 &  \\\\\n",
    " &  \\vdots  & \\\\\n",
    " & y_i &  \\\\\n",
    " &  \\vdots  & \\\\\n",
    " & y_m &  \\\\\n",
    "\\end{bmatrix} \\in \\mathbb{R}^m $$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c436efb-43ab-449e-8094-5e8ae47e662e",
   "metadata": {},
   "source": [
    "It's very simple to prove that : \n",
    "\n",
    "$$ J(w) = \\frac{1}{2} \\left \\lVert Xw - y \\right \\rVert_{2}^{2} = \\frac{1}{2m} \\left( Xw - y\\right)^T\\left( Xw - y\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55fa0f7-2f1e-4090-9214-bb7922895005",
   "metadata": {},
   "source": [
    "so, \n",
    "\n",
    "$$ \\frac{\\partial J(w)}{\\partial w} =  \\frac{\\partial}{\\partial w}  \\frac{1}{2m} \\left( Xw - y\\right)^T.\\left( Xw - y\\right) $$\n",
    "\n",
    "$$ =  \\frac{1}{2m} \\frac{\\partial}{\\partial w} \\left( Xw - y\\right)^T.\\left( Xw - y\\right) $$\n",
    "\n",
    "$$  =  \\frac{1}{2m} \\frac{\\partial}{\\partial w} \\left( (Xw)^T - y^T \\right).\\left( Xw - y\\right) $$\n",
    "\n",
    "$$  =  \\frac{1}{2m} \\frac{\\partial}{\\partial w} \\left( (Xw)^TXw - (Xw)^Ty - y^T(Xw) + y^Ty \\right) \\mid \\text{ where } y^Ty = cst$$\n",
    "\n",
    "$$  =  \\frac{1}{2m} \\frac{\\partial}{\\partial w} \\left( w^T(X^TX)w - (Xw)^Ty - y^T(Xw) \\right) \\mid  \\text{ where } (Xw)^Ty =  y^T(Xw) $$\n",
    "\n",
    "$$  =  \\frac{1}{2m} \\frac{\\partial}{\\partial w} \\left( w^T(X^TX)w - 2(X^Ty)w \\right) \\hspace {1cm} (*) $$ \n",
    "\n",
    "$$  =  \\frac{1}{2m} \\left( 2X^TXw - 2X^Ty \\right) $$ \n",
    "\n",
    "$$  =  \\frac{1}{m} \\left( X^TXw - X^Ty \\right) $$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b2bcf5-016f-4d8d-b6cd-40dd6f4b0c5a",
   "metadata": {},
   "source": [
    "finally, \n",
    "$$ \\frac{\\partial J(w)}{\\partial w} = \\frac{1}{m} \\left( X^TXw - X^Ty \\right) $$\n",
    "\n",
    "so, \n",
    "\n",
    "$$ \\frac{\\partial J(w)}{\\partial w} = 0 \\Longleftrightarrow X^TXw^* - X^Ty = 0 $$\n",
    "\n",
    "$$ \\Longleftrightarrow X^TXw^* = X^Ty $$\n",
    "\n",
    "Thus the value of $w^*$ that minimize $J(w)$ s given in closed form by the equation : \n",
    "\n",
    "$$ w^* = (X^TX)^{-1}X^Ty $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcf4d85-6f61-45fc-8647-b3501ae26291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
