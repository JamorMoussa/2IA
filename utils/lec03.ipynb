{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "EPhkfsEQ-dGM"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nOCxwOtTKV7P"
   },
   "source": [
    "Let's consider the prediction problem from Q4 of HW1. We are given an input $x \\in \\{0,1\\}$, and we are trying to predict a label $t \\in \\{0, 1\\}$. For this problem, we assumed that the data generating distribution $p_{data}$ was any distribution such that $p_{data}(x) > 0$ for all $x \\in \\{0,1\\}$. Let's consider a special case of $p_{data}$, which is given by the following table.\n",
    "\n",
    " &nbsp; | `x = 0` | `x = 1`\n",
    "--- | --- | ---\n",
    "**`t = 0`** | 9/20 | 2/20\n",
    "**`t = 1`** | 6/20 | 3/20\n",
    "\n",
    "We assume that our data is random pairs $(x, t) \\sim p_{data}(x,t)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "XUOgnJ1h-g4m"
   },
   "outputs": [],
   "source": [
    "def data_gen_joint(x, t):\n",
    "  \"\"\"Returns the data generating joint distribution p_data(x,t).\"\"\"\n",
    "  if x == 0 and t == 0:\n",
    "    return 9/20\n",
    "  elif x == 0 and t == 1:\n",
    "    return 6/20\n",
    "  elif x == 1 and t == 0:\n",
    "    return 2/20\n",
    "  elif x == 1 and t == 1:\n",
    "    return 3/20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1Oz4oBDL-gX"
   },
   "source": [
    "Based on this joint distribution, we can compute the relevant marginal $p_{data}(x)$ and conditional $p_{data}(t | x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "51ztAUIUB18S"
   },
   "outputs": [],
   "source": [
    "def data_gen_marginal(x):\n",
    "  \"\"\"Returns the data generating marginal distribution p_data(x).\n",
    "  \n",
    "  p_data(x) = sum_t p_data(x, t)\n",
    "  \"\"\"\n",
    "  px = 0\n",
    "  for t in [0, 1]:\n",
    "    px = px + data_gen_joint(x, t)\n",
    "  return px\n",
    "\n",
    "def data_gen_conditional(t, x):\n",
    "  \"\"\"Returns the data generating conditional distribution p_data(t | x).\n",
    "  \n",
    "    p_data(t | x) = p_data(x,t) / p_data(x)\n",
    "  \"\"\"\n",
    "  return data_gen_joint(x, t) / data_gen_marginal(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KpI2mhO_MEWh"
   },
   "source": [
    "Let us consider predictors, which are functions $y : \\{0, 1\\} \\to \\{0, 1\\}$. It is not too hard to convince yourself that there are *only* 4 possible predictors:\n",
    "\n",
    " &nbsp; &nbsp; &nbsp; &nbsp;| $y_i(0)$ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; |$y_i(1)$   &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; \n",
    "--- | --- | ---\n",
    "$y_1$ | 0 | 1\n",
    "$y_2$ | 1 | 0\n",
    "$y_3$ | 0 | 0\n",
    "$y_4$ | 1 | 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "UUkH00ShAII_",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_1: y_1(0)=0 y_1(1)=1\n",
      "y_2: y_2(0)=1 y_2(1)=0\n",
      "y_3: y_3(0)=0 y_3(1)=0\n",
      "y_4: y_4(0)=1 y_4(1)=1\n",
      "y_star: y_star(0)=0 y_star(1)=1\n"
     ]
    }
   ],
   "source": [
    "def y_1(x):\n",
    "  return x\n",
    "\n",
    "def y_2(x):\n",
    "  return 1 - x\n",
    "\n",
    "def y_3(x):\n",
    "  return 0\n",
    "\n",
    "def y_4(x):\n",
    "  return 1\n",
    "\n",
    "\n",
    "def y_star(x):\n",
    "    \"\"\"\n",
    "    This computes the optimal predictor\n",
    "    \n",
    "    y*(x) = arg max_{t in {0, 1}} p_data(t | x)\n",
    "    \"\"\"\n",
    "    # Recall that t is either 0 or 1\n",
    "    if data_gen_conditional(0, x) > data_gen_conditional(1, x):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "all_predictors = [y_1, y_2, y_3, y_4]\n",
    "\n",
    "for i, y in enumerate(all_predictors):\n",
    "    print(f\"y_{i+1}: y_{i+1}(0)={y(0)} y_{i+1}(1)={y(1)}\")\n",
    "    \n",
    "print(f\"y_star: y_star(0)={y_star(0)} y_star(1)={y_star(1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3V4KYLVtNseD"
   },
   "source": [
    "We will score predictors based on the 0-1 loss:\n",
    "\n",
    "$$L(y, t) = \\begin{cases} 0 & \\text{if } y = t\\\\1 &\\text{if } y \\neq t\\end{cases}$$\n",
    "\n",
    "Given a predictor $y$, we can compute the expected loss on the data generating distribuition:\n",
    "\n",
    "$$\\mathcal{R}[y] = \\mathbb{E}_{(X,T) \\sim p_{data}}[L(y(X), T)] = \\sum_{t \\in \\{0,1\\}} \\sum_{x \\in \\{0,1\\}} p_{data}(x,t) L(y(x), t)$$\n",
    "\n",
    "Given a predictor $y$ and a data set $\\mathcal{D}$ with $N$ data points, we can compute the average loss:\n",
    "\n",
    "$$\\hat{\\mathcal{R}}[y, \\mathcal{D}] = \\sum_{(x^{(i)}, t^{(i)}) \\in \\mathcal{D}} \\frac{1}{N} L(y(x^{(i)}), t^{(i)})$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ugqzmVM8AGyX"
   },
   "outputs": [],
   "source": [
    "def loss(y, t):\n",
    "  \"\"\"The 0-1 loss function.\"\"\"\n",
    "  return int(y != t)\n",
    "\n",
    "def expected_loss(y, p_data):\n",
    "  \"\"\"Compute the expected loss of y : {0,1} -> {0,1}.\"\"\"\n",
    "  L = 0\n",
    "  for t in [0, 1]:\n",
    "    for x in [0, 1]:\n",
    "      L = L + p_data(x, t) * loss(y(x), t)\n",
    "  return L\n",
    "\n",
    "def average_loss(y, D):\n",
    "  \"\"\"Compute the average loss of y : {0,1} -> {0,1}.\"\"\"\n",
    "  N = len(D)\n",
    "  L = 0\n",
    "  for (xi, ti) in D:\n",
    "    L = L + loss(y(xi), ti) \n",
    "  return L / N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghjiBnoLO1JV"
   },
   "source": [
    "Let's evaluate the predictors $y_i$ on expected loss $\\mathcal{R}[y_i]$ under the data generating distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fB356CKtA41_",
    "outputId": "fad7b1be-1317-4641-bd32-646b14329deb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R[y_1]=0.40\n",
      "R[y_2]=0.60\n",
      "R[y_3]=0.45\n",
      "R[y_4]=0.55\n",
      "Which is the optimal predictor in terms of expected loss? y_1\n"
     ]
    }
   ],
   "source": [
    "predictor_losses = [expected_loss(y, data_gen_joint) for y in all_predictors]\n",
    "for i, predictor_loss in enumerate(predictor_losses):\n",
    "  print(f\"R[y_{i+1}]=\" + \"{:.2f}\".format(predictor_loss))\n",
    "\n",
    "# Report\n",
    "print(f\"Which is the optimal predictor in terms of expected loss? y_{np.argmin(predictor_losses)+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBfR5BMXO53n"
   },
   "source": [
    "Notice that there is a *unique* optimal predictor: $y_1$. This predictor is the same as $y^{\\star}$. Let's consider how hard it is to *learn* this predictor from a finite training set. First, let's write a function for generating data sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "55SeXon9DtHV"
   },
   "outputs": [],
   "source": [
    "def sample_data_set(N):\n",
    "  \"\"\"Sample a data set according to the data generating distribution\"\"\"\n",
    "  D = []\n",
    "  for i in range(N):\n",
    "    # First, sample x.\n",
    "    # px = [p(X=0), p(X=1)]\n",
    "    px = [data_gen_marginal(0), data_gen_marginal(1)]\n",
    "    x = np.random.choice([0, 1], p=px)\n",
    "\n",
    "    # Now we sample t given x.\n",
    "    # pt_given_x = [p(T=0 | X=x), p(T=1 | X=x)]\n",
    "    pt_given_x = [data_gen_conditional(0, x), data_gen_conditional(1, x)]\n",
    "    t = np.random.choice([0,1], p=pt_given_x)\n",
    "\n",
    "    # Add (x,t) to training set\n",
    "    D.append((x, t))\n",
    "  return D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lmKwy1T0PG-j"
   },
   "source": [
    "Let's sample a finite training set $\\mathcal{D}^{train}$, evaluate all of the predictors in terms of average loss $\\hat{\\mathcal{R}}[y_i, \\mathcal{D}^{train}]$ on the training set and pick the best one $\\hat{y}^{\\star}$. Which one did we pick?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XAKhtOkKBt-C",
    "outputId": "2e79fc69-5a8b-478f-b426-dba39d16830b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set\n",
      "x_0: 0  t_0: 0\n",
      "x_1: 1  t_1: 0\n",
      "x_2: 0  t_2: 1\n",
      "x_3: 1  t_3: 0\n",
      "x_4: 0  t_4: 0\n",
      "x_5: 1  t_5: 1\n",
      "x_6: 1  t_6: 1\n",
      "x_7: 0  t_7: 0\n",
      "x_8: 0  t_8: 0\n",
      "x_9: 1  t_9: 0\n",
      "x_10: 1  t_10: 0\n",
      "x_11: 0  t_11: 1\n",
      "x_12: 0  t_12: 0\n",
      "x_13: 0  t_13: 0\n",
      "x_14: 1  t_14: 0\n",
      "x_15: 1  t_15: 1\n",
      "x_16: 0  t_16: 1\n",
      "x_17: 0  t_17: 1\n",
      "x_18: 0  t_18: 1\n",
      "x_19: 0  t_19: 0\n",
      "x_20: 0  t_20: 1\n",
      "x_21: 1  t_21: 1\n",
      "x_22: 1  t_22: 1\n",
      "x_23: 1  t_23: 1\n",
      "x_24: 0  t_24: 0\n",
      "x_25: 0  t_25: 1\n",
      "x_26: 1  t_26: 0\n",
      "x_27: 0  t_27: 0\n",
      "x_28: 1  t_28: 1\n",
      "x_29: 0  t_29: 1\n",
      "x_30: 0  t_30: 0\n",
      "x_31: 0  t_31: 1\n",
      "x_32: 1  t_32: 1\n",
      "x_33: 0  t_33: 0\n",
      "x_34: 0  t_34: 0\n",
      "x_35: 0  t_35: 0\n",
      "x_36: 0  t_36: 0\n",
      "x_37: 0  t_37: 0\n",
      "x_38: 0  t_38: 1\n",
      "x_39: 0  t_39: 0\n",
      "x_40: 1  t_40: 1\n",
      "x_41: 0  t_41: 1\n",
      "x_42: 0  t_42: 0\n",
      "x_43: 0  t_43: 1\n",
      "x_44: 0  t_44: 0\n",
      "x_45: 0  t_45: 1\n",
      "x_46: 0  t_46: 0\n",
      "x_47: 0  t_47: 0\n",
      "x_48: 0  t_48: 0\n",
      "x_49: 0  t_49: 0\n",
      "x_50: 0  t_50: 1\n",
      "x_51: 0  t_51: 1\n",
      "x_52: 0  t_52: 1\n",
      "x_53: 1  t_53: 0\n",
      "x_54: 0  t_54: 0\n",
      "x_55: 0  t_55: 0\n",
      "x_56: 1  t_56: 0\n",
      "x_57: 0  t_57: 0\n",
      "x_58: 1  t_58: 1\n",
      "x_59: 1  t_59: 1\n",
      "x_60: 0  t_60: 1\n",
      "x_61: 1  t_61: 1\n",
      "x_62: 1  t_62: 0\n",
      "x_63: 0  t_63: 0\n",
      "x_64: 0  t_64: 0\n",
      "x_65: 0  t_65: 0\n",
      "x_66: 1  t_66: 0\n",
      "x_67: 0  t_67: 1\n",
      "x_68: 0  t_68: 0\n",
      "x_69: 0  t_69: 0\n",
      "x_70: 0  t_70: 0\n",
      "x_71: 1  t_71: 0\n",
      "x_72: 0  t_72: 1\n",
      "x_73: 0  t_73: 0\n",
      "x_74: 0  t_74: 0\n",
      "x_75: 0  t_75: 1\n",
      "x_76: 0  t_76: 1\n",
      "x_77: 1  t_77: 0\n",
      "x_78: 0  t_78: 1\n",
      "x_79: 1  t_79: 0\n",
      "x_80: 0  t_80: 0\n",
      "x_81: 0  t_81: 1\n",
      "x_82: 1  t_82: 1\n",
      "x_83: 0  t_83: 1\n",
      "x_84: 0  t_84: 1\n",
      "x_85: 1  t_85: 1\n",
      "x_86: 0  t_86: 0\n",
      "x_87: 0  t_87: 1\n",
      "x_88: 0  t_88: 0\n",
      "x_89: 1  t_89: 0\n",
      "x_90: 0  t_90: 0\n",
      "x_91: 0  t_91: 0\n",
      "x_92: 0  t_92: 1\n",
      "x_93: 0  t_93: 0\n",
      "x_94: 1  t_94: 0\n",
      "x_95: 1  t_95: 1\n",
      "x_96: 1  t_96: 1\n",
      "x_97: 0  t_97: 0\n",
      "x_98: 1  t_98: 0\n",
      "x_99: 0  t_99: 1\n",
      "x_100: 0  t_100: 0\n",
      "x_101: 1  t_101: 1\n",
      "x_102: 0  t_102: 0\n",
      "x_103: 1  t_103: 0\n",
      "x_104: 1  t_104: 0\n",
      "x_105: 0  t_105: 0\n",
      "x_106: 0  t_106: 0\n",
      "x_107: 0  t_107: 1\n",
      "x_108: 0  t_108: 1\n",
      "x_109: 0  t_109: 1\n",
      "x_110: 0  t_110: 1\n",
      "x_111: 1  t_111: 1\n",
      "x_112: 1  t_112: 1\n",
      "x_113: 1  t_113: 1\n",
      "x_114: 1  t_114: 1\n",
      "x_115: 0  t_115: 0\n",
      "x_116: 0  t_116: 0\n",
      "x_117: 0  t_117: 1\n",
      "x_118: 1  t_118: 1\n",
      "x_119: 0  t_119: 1\n",
      "x_120: 0  t_120: 1\n",
      "x_121: 0  t_121: 0\n",
      "x_122: 1  t_122: 0\n",
      "x_123: 0  t_123: 0\n",
      "x_124: 0  t_124: 0\n",
      "x_125: 0  t_125: 0\n",
      "x_126: 0  t_126: 1\n",
      "x_127: 0  t_127: 1\n",
      "x_128: 1  t_128: 1\n",
      "x_129: 0  t_129: 0\n",
      "x_130: 0  t_130: 0\n",
      "x_131: 0  t_131: 0\n",
      "x_132: 0  t_132: 0\n",
      "x_133: 0  t_133: 1\n",
      "x_134: 0  t_134: 0\n",
      "x_135: 0  t_135: 0\n",
      "x_136: 0  t_136: 0\n",
      "x_137: 0  t_137: 1\n",
      "x_138: 0  t_138: 1\n",
      "x_139: 0  t_139: 1\n",
      "x_140: 0  t_140: 1\n",
      "x_141: 1  t_141: 1\n",
      "x_142: 0  t_142: 1\n",
      "x_143: 1  t_143: 0\n",
      "x_144: 0  t_144: 1\n",
      "x_145: 0  t_145: 1\n",
      "x_146: 0  t_146: 0\n",
      "x_147: 1  t_147: 1\n",
      "x_148: 0  t_148: 1\n",
      "x_149: 1  t_149: 1\n",
      "x_150: 1  t_150: 1\n",
      "x_151: 1  t_151: 1\n",
      "x_152: 0  t_152: 0\n",
      "x_153: 0  t_153: 1\n",
      "x_154: 0  t_154: 0\n",
      "x_155: 0  t_155: 0\n",
      "x_156: 0  t_156: 1\n",
      "x_157: 0  t_157: 0\n",
      "x_158: 1  t_158: 1\n",
      "x_159: 0  t_159: 0\n",
      "x_160: 0  t_160: 0\n",
      "x_161: 0  t_161: 1\n",
      "x_162: 1  t_162: 1\n",
      "x_163: 0  t_163: 1\n",
      "x_164: 1  t_164: 0\n",
      "x_165: 0  t_165: 1\n",
      "x_166: 0  t_166: 0\n",
      "x_167: 0  t_167: 0\n",
      "x_168: 0  t_168: 0\n",
      "x_169: 1  t_169: 1\n",
      "x_170: 0  t_170: 1\n",
      "x_171: 0  t_171: 0\n",
      "x_172: 0  t_172: 0\n",
      "x_173: 0  t_173: 1\n",
      "x_174: 0  t_174: 0\n",
      "x_175: 0  t_175: 1\n",
      "x_176: 0  t_176: 1\n",
      "x_177: 0  t_177: 0\n",
      "x_178: 0  t_178: 1\n",
      "x_179: 1  t_179: 1\n",
      "x_180: 0  t_180: 0\n",
      "x_181: 1  t_181: 0\n",
      "x_182: 0  t_182: 1\n",
      "x_183: 1  t_183: 0\n",
      "x_184: 1  t_184: 1\n",
      "x_185: 0  t_185: 0\n",
      "x_186: 0  t_186: 0\n",
      "x_187: 0  t_187: 0\n",
      "x_188: 0  t_188: 0\n",
      "x_189: 0  t_189: 0\n",
      "x_190: 0  t_190: 0\n",
      "x_191: 0  t_191: 0\n",
      "x_192: 0  t_192: 0\n",
      "x_193: 0  t_193: 0\n",
      "x_194: 0  t_194: 0\n",
      "x_195: 1  t_195: 0\n",
      "x_196: 1  t_196: 1\n",
      "x_197: 1  t_197: 1\n",
      "x_198: 1  t_198: 1\n",
      "x_199: 0  t_199: 0\n",
      "x_200: 0  t_200: 1\n",
      "x_201: 0  t_201: 0\n",
      "x_202: 0  t_202: 0\n",
      "x_203: 0  t_203: 1\n",
      "x_204: 0  t_204: 0\n",
      "x_205: 0  t_205: 0\n",
      "x_206: 0  t_206: 0\n",
      "x_207: 0  t_207: 0\n",
      "x_208: 0  t_208: 0\n",
      "x_209: 0  t_209: 1\n",
      "x_210: 0  t_210: 1\n",
      "x_211: 1  t_211: 1\n",
      "x_212: 0  t_212: 1\n",
      "x_213: 1  t_213: 1\n",
      "x_214: 0  t_214: 1\n",
      "x_215: 0  t_215: 1\n",
      "x_216: 0  t_216: 1\n",
      "x_217: 0  t_217: 0\n",
      "x_218: 0  t_218: 0\n",
      "x_219: 1  t_219: 1\n",
      "x_220: 0  t_220: 1\n",
      "x_221: 0  t_221: 0\n",
      "x_222: 0  t_222: 1\n",
      "x_223: 0  t_223: 0\n",
      "x_224: 0  t_224: 0\n",
      "x_225: 0  t_225: 0\n",
      "x_226: 0  t_226: 0\n",
      "x_227: 0  t_227: 1\n",
      "x_228: 1  t_228: 1\n",
      "x_229: 0  t_229: 1\n",
      "x_230: 0  t_230: 0\n",
      "x_231: 0  t_231: 1\n",
      "x_232: 0  t_232: 0\n",
      "x_233: 1  t_233: 0\n",
      "x_234: 0  t_234: 0\n",
      "x_235: 0  t_235: 1\n",
      "x_236: 0  t_236: 0\n",
      "x_237: 0  t_237: 0\n",
      "x_238: 0  t_238: 1\n",
      "x_239: 0  t_239: 1\n",
      "x_240: 1  t_240: 1\n",
      "x_241: 0  t_241: 0\n",
      "x_242: 1  t_242: 1\n",
      "x_243: 0  t_243: 0\n",
      "x_244: 1  t_244: 1\n",
      "x_245: 0  t_245: 0\n",
      "x_246: 1  t_246: 1\n",
      "x_247: 0  t_247: 0\n",
      "x_248: 0  t_248: 0\n",
      "x_249: 0  t_249: 0\n",
      "x_250: 1  t_250: 0\n",
      "x_251: 0  t_251: 1\n",
      "x_252: 0  t_252: 0\n",
      "x_253: 0  t_253: 0\n",
      "x_254: 0  t_254: 1\n",
      "x_255: 1  t_255: 1\n",
      "x_256: 0  t_256: 0\n",
      "x_257: 1  t_257: 0\n",
      "x_258: 0  t_258: 0\n",
      "x_259: 0  t_259: 1\n",
      "x_260: 0  t_260: 1\n",
      "x_261: 1  t_261: 0\n",
      "x_262: 0  t_262: 1\n",
      "x_263: 0  t_263: 0\n",
      "x_264: 0  t_264: 0\n",
      "x_265: 0  t_265: 1\n",
      "x_266: 0  t_266: 0\n",
      "x_267: 0  t_267: 1\n",
      "x_268: 0  t_268: 0\n",
      "x_269: 0  t_269: 1\n",
      "x_270: 1  t_270: 0\n",
      "x_271: 0  t_271: 1\n",
      "x_272: 1  t_272: 0\n",
      "x_273: 0  t_273: 0\n",
      "x_274: 1  t_274: 0\n",
      "x_275: 0  t_275: 1\n",
      "x_276: 0  t_276: 0\n",
      "x_277: 0  t_277: 0\n",
      "x_278: 0  t_278: 0\n",
      "x_279: 0  t_279: 1\n",
      "x_280: 0  t_280: 0\n",
      "x_281: 1  t_281: 1\n",
      "x_282: 0  t_282: 0\n",
      "x_283: 0  t_283: 0\n",
      "x_284: 0  t_284: 0\n",
      "x_285: 0  t_285: 0\n",
      "x_286: 0  t_286: 1\n",
      "x_287: 0  t_287: 0\n",
      "x_288: 0  t_288: 0\n",
      "x_289: 0  t_289: 1\n",
      "x_290: 0  t_290: 0\n",
      "x_291: 0  t_291: 1\n",
      "x_292: 1  t_292: 1\n",
      "x_293: 0  t_293: 1\n",
      "x_294: 1  t_294: 0\n",
      "x_295: 1  t_295: 1\n",
      "x_296: 1  t_296: 1\n",
      "x_297: 0  t_297: 1\n",
      "x_298: 0  t_298: 1\n",
      "x_299: 0  t_299: 0\n",
      "x_300: 0  t_300: 1\n",
      "x_301: 0  t_301: 0\n",
      "x_302: 1  t_302: 1\n",
      "x_303: 0  t_303: 1\n",
      "x_304: 0  t_304: 1\n",
      "x_305: 0  t_305: 0\n",
      "x_306: 0  t_306: 0\n",
      "x_307: 1  t_307: 0\n",
      "x_308: 1  t_308: 0\n",
      "x_309: 0  t_309: 1\n",
      "x_310: 0  t_310: 0\n",
      "x_311: 0  t_311: 1\n",
      "x_312: 0  t_312: 0\n",
      "x_313: 0  t_313: 0\n",
      "x_314: 0  t_314: 0\n",
      "x_315: 0  t_315: 0\n",
      "x_316: 0  t_316: 0\n",
      "x_317: 0  t_317: 1\n",
      "x_318: 0  t_318: 0\n",
      "x_319: 0  t_319: 0\n",
      "x_320: 0  t_320: 0\n",
      "x_321: 0  t_321: 0\n",
      "x_322: 0  t_322: 1\n",
      "x_323: 0  t_323: 0\n",
      "x_324: 0  t_324: 1\n",
      "x_325: 0  t_325: 1\n",
      "x_326: 0  t_326: 1\n",
      "x_327: 1  t_327: 0\n",
      "x_328: 0  t_328: 0\n",
      "x_329: 0  t_329: 0\n",
      "x_330: 1  t_330: 0\n",
      "x_331: 0  t_331: 0\n",
      "x_332: 0  t_332: 0\n",
      "x_333: 0  t_333: 1\n",
      "x_334: 1  t_334: 0\n",
      "x_335: 1  t_335: 1\n",
      "x_336: 1  t_336: 1\n",
      "x_337: 0  t_337: 1\n",
      "x_338: 0  t_338: 0\n",
      "x_339: 1  t_339: 0\n",
      "x_340: 1  t_340: 0\n",
      "x_341: 0  t_341: 1\n",
      "x_342: 1  t_342: 1\n",
      "x_343: 0  t_343: 0\n",
      "x_344: 0  t_344: 1\n",
      "x_345: 0  t_345: 1\n",
      "x_346: 0  t_346: 1\n",
      "x_347: 0  t_347: 0\n",
      "x_348: 0  t_348: 1\n",
      "x_349: 0  t_349: 0\n",
      "x_350: 0  t_350: 1\n",
      "x_351: 0  t_351: 1\n",
      "x_352: 1  t_352: 1\n",
      "x_353: 0  t_353: 0\n",
      "x_354: 0  t_354: 1\n",
      "x_355: 1  t_355: 1\n",
      "x_356: 0  t_356: 1\n",
      "x_357: 0  t_357: 0\n",
      "x_358: 1  t_358: 0\n",
      "x_359: 0  t_359: 0\n",
      "x_360: 0  t_360: 0\n",
      "x_361: 0  t_361: 1\n",
      "x_362: 0  t_362: 1\n",
      "x_363: 1  t_363: 1\n",
      "x_364: 0  t_364: 0\n",
      "x_365: 0  t_365: 1\n",
      "x_366: 0  t_366: 0\n",
      "x_367: 0  t_367: 1\n",
      "x_368: 0  t_368: 1\n",
      "x_369: 0  t_369: 0\n",
      "x_370: 0  t_370: 1\n",
      "x_371: 0  t_371: 1\n",
      "x_372: 0  t_372: 0\n",
      "x_373: 0  t_373: 0\n",
      "x_374: 0  t_374: 1\n",
      "x_375: 0  t_375: 1\n",
      "x_376: 0  t_376: 0\n",
      "x_377: 0  t_377: 1\n",
      "x_378: 0  t_378: 0\n",
      "x_379: 1  t_379: 1\n",
      "x_380: 0  t_380: 0\n",
      "x_381: 0  t_381: 0\n",
      "x_382: 1  t_382: 1\n",
      "x_383: 0  t_383: 0\n",
      "x_384: 0  t_384: 1\n",
      "x_385: 0  t_385: 1\n",
      "x_386: 1  t_386: 1\n",
      "x_387: 0  t_387: 1\n",
      "x_388: 1  t_388: 0\n",
      "x_389: 1  t_389: 1\n",
      "x_390: 0  t_390: 1\n",
      "x_391: 0  t_391: 1\n",
      "x_392: 0  t_392: 1\n",
      "x_393: 0  t_393: 1\n",
      "x_394: 0  t_394: 0\n",
      "x_395: 0  t_395: 0\n",
      "x_396: 0  t_396: 0\n",
      "x_397: 0  t_397: 0\n",
      "x_398: 0  t_398: 0\n",
      "x_399: 1  t_399: 1\n",
      "x_400: 0  t_400: 0\n",
      "x_401: 0  t_401: 1\n",
      "x_402: 1  t_402: 1\n",
      "x_403: 1  t_403: 1\n",
      "x_404: 1  t_404: 0\n",
      "x_405: 0  t_405: 0\n",
      "x_406: 0  t_406: 1\n",
      "x_407: 1  t_407: 0\n",
      "x_408: 0  t_408: 0\n",
      "x_409: 1  t_409: 1\n",
      "x_410: 0  t_410: 0\n",
      "x_411: 1  t_411: 1\n",
      "x_412: 0  t_412: 1\n",
      "x_413: 0  t_413: 1\n",
      "x_414: 0  t_414: 0\n",
      "x_415: 1  t_415: 1\n",
      "x_416: 0  t_416: 1\n",
      "x_417: 0  t_417: 0\n",
      "x_418: 0  t_418: 1\n",
      "x_419: 0  t_419: 0\n",
      "x_420: 1  t_420: 1\n",
      "x_421: 0  t_421: 0\n",
      "x_422: 0  t_422: 0\n",
      "x_423: 0  t_423: 0\n",
      "x_424: 0  t_424: 0\n",
      "x_425: 0  t_425: 0\n",
      "x_426: 0  t_426: 0\n",
      "x_427: 0  t_427: 1\n",
      "x_428: 0  t_428: 0\n",
      "x_429: 0  t_429: 0\n",
      "x_430: 1  t_430: 1\n",
      "x_431: 0  t_431: 0\n",
      "x_432: 0  t_432: 0\n",
      "x_433: 1  t_433: 1\n",
      "x_434: 1  t_434: 0\n",
      "x_435: 0  t_435: 0\n",
      "x_436: 0  t_436: 1\n",
      "x_437: 0  t_437: 0\n",
      "x_438: 0  t_438: 1\n",
      "x_439: 0  t_439: 0\n",
      "x_440: 0  t_440: 1\n",
      "x_441: 1  t_441: 0\n",
      "x_442: 1  t_442: 0\n",
      "x_443: 0  t_443: 1\n",
      "x_444: 1  t_444: 0\n",
      "x_445: 1  t_445: 1\n",
      "x_446: 0  t_446: 0\n",
      "x_447: 0  t_447: 0\n",
      "x_448: 0  t_448: 0\n",
      "x_449: 1  t_449: 1\n",
      "x_450: 0  t_450: 0\n",
      "x_451: 0  t_451: 1\n",
      "x_452: 1  t_452: 1\n",
      "x_453: 0  t_453: 1\n",
      "x_454: 0  t_454: 0\n",
      "x_455: 0  t_455: 0\n",
      "x_456: 1  t_456: 1\n",
      "x_457: 0  t_457: 0\n",
      "x_458: 1  t_458: 1\n",
      "x_459: 1  t_459: 0\n",
      "x_460: 0  t_460: 0\n",
      "x_461: 0  t_461: 0\n",
      "x_462: 1  t_462: 1\n",
      "x_463: 0  t_463: 0\n",
      "x_464: 0  t_464: 0\n",
      "x_465: 0  t_465: 1\n",
      "x_466: 0  t_466: 0\n",
      "x_467: 0  t_467: 1\n",
      "x_468: 0  t_468: 0\n",
      "x_469: 0  t_469: 0\n",
      "x_470: 1  t_470: 1\n",
      "x_471: 0  t_471: 1\n",
      "x_472: 0  t_472: 0\n",
      "x_473: 0  t_473: 0\n",
      "x_474: 1  t_474: 1\n",
      "x_475: 0  t_475: 0\n",
      "x_476: 1  t_476: 1\n",
      "x_477: 0  t_477: 1\n",
      "x_478: 0  t_478: 0\n",
      "x_479: 0  t_479: 1\n",
      "x_480: 1  t_480: 0\n",
      "x_481: 1  t_481: 0\n",
      "x_482: 0  t_482: 0\n",
      "x_483: 1  t_483: 1\n",
      "x_484: 0  t_484: 0\n",
      "x_485: 0  t_485: 0\n",
      "x_486: 0  t_486: 1\n",
      "x_487: 0  t_487: 0\n",
      "x_488: 0  t_488: 0\n",
      "x_489: 0  t_489: 0\n",
      "x_490: 0  t_490: 0\n",
      "x_491: 0  t_491: 0\n",
      "x_492: 0  t_492: 0\n",
      "x_493: 1  t_493: 0\n",
      "x_494: 1  t_494: 0\n",
      "x_495: 0  t_495: 0\n",
      "x_496: 0  t_496: 0\n",
      "x_497: 0  t_497: 1\n",
      "x_498: 0  t_498: 1\n",
      "x_499: 0  t_499: 1\n",
      "x_500: 0  t_500: 1\n",
      "x_501: 0  t_501: 0\n",
      "x_502: 0  t_502: 0\n",
      "x_503: 0  t_503: 0\n",
      "x_504: 0  t_504: 0\n",
      "x_505: 0  t_505: 0\n",
      "x_506: 1  t_506: 1\n",
      "x_507: 0  t_507: 1\n",
      "x_508: 0  t_508: 1\n",
      "x_509: 0  t_509: 0\n",
      "x_510: 0  t_510: 1\n",
      "x_511: 0  t_511: 0\n",
      "x_512: 0  t_512: 1\n",
      "x_513: 0  t_513: 0\n",
      "x_514: 0  t_514: 1\n",
      "x_515: 0  t_515: 0\n",
      "x_516: 0  t_516: 1\n",
      "x_517: 0  t_517: 0\n",
      "x_518: 1  t_518: 1\n",
      "x_519: 0  t_519: 0\n",
      "x_520: 0  t_520: 0\n",
      "x_521: 0  t_521: 1\n",
      "x_522: 1  t_522: 1\n",
      "x_523: 0  t_523: 0\n",
      "x_524: 0  t_524: 1\n",
      "x_525: 0  t_525: 0\n",
      "x_526: 0  t_526: 0\n",
      "x_527: 1  t_527: 1\n",
      "x_528: 0  t_528: 0\n",
      "x_529: 0  t_529: 0\n",
      "x_530: 0  t_530: 1\n",
      "x_531: 0  t_531: 0\n",
      "x_532: 0  t_532: 0\n",
      "x_533: 1  t_533: 0\n",
      "x_534: 0  t_534: 1\n",
      "x_535: 0  t_535: 0\n",
      "x_536: 0  t_536: 1\n",
      "x_537: 0  t_537: 0\n",
      "x_538: 0  t_538: 0\n",
      "x_539: 0  t_539: 0\n",
      "x_540: 0  t_540: 1\n",
      "x_541: 0  t_541: 0\n",
      "x_542: 0  t_542: 0\n",
      "x_543: 0  t_543: 0\n",
      "x_544: 0  t_544: 1\n",
      "x_545: 0  t_545: 0\n",
      "x_546: 0  t_546: 0\n",
      "x_547: 0  t_547: 0\n",
      "x_548: 1  t_548: 0\n",
      "x_549: 0  t_549: 0\n",
      "x_550: 1  t_550: 0\n",
      "x_551: 0  t_551: 1\n",
      "x_552: 0  t_552: 1\n",
      "x_553: 0  t_553: 1\n",
      "x_554: 0  t_554: 1\n",
      "x_555: 0  t_555: 0\n",
      "x_556: 0  t_556: 0\n",
      "x_557: 0  t_557: 0\n",
      "x_558: 0  t_558: 0\n",
      "x_559: 0  t_559: 0\n",
      "x_560: 0  t_560: 0\n",
      "x_561: 1  t_561: 1\n",
      "x_562: 0  t_562: 1\n",
      "x_563: 0  t_563: 1\n",
      "x_564: 0  t_564: 1\n",
      "x_565: 1  t_565: 0\n",
      "x_566: 1  t_566: 0\n",
      "x_567: 0  t_567: 1\n",
      "x_568: 1  t_568: 1\n",
      "x_569: 0  t_569: 0\n",
      "x_570: 0  t_570: 1\n",
      "x_571: 1  t_571: 1\n",
      "x_572: 0  t_572: 1\n",
      "x_573: 0  t_573: 0\n",
      "x_574: 0  t_574: 1\n",
      "x_575: 0  t_575: 0\n",
      "x_576: 0  t_576: 0\n",
      "x_577: 0  t_577: 1\n",
      "x_578: 0  t_578: 0\n",
      "x_579: 1  t_579: 1\n",
      "x_580: 1  t_580: 1\n",
      "x_581: 0  t_581: 0\n",
      "x_582: 0  t_582: 0\n",
      "x_583: 0  t_583: 0\n",
      "x_584: 0  t_584: 0\n",
      "x_585: 1  t_585: 1\n",
      "x_586: 0  t_586: 0\n",
      "x_587: 0  t_587: 0\n",
      "x_588: 0  t_588: 1\n",
      "x_589: 0  t_589: 0\n",
      "x_590: 0  t_590: 0\n",
      "x_591: 1  t_591: 0\n",
      "x_592: 0  t_592: 1\n",
      "x_593: 0  t_593: 1\n",
      "x_594: 1  t_594: 1\n",
      "x_595: 0  t_595: 1\n",
      "x_596: 0  t_596: 1\n",
      "x_597: 1  t_597: 0\n",
      "x_598: 0  t_598: 0\n",
      "x_599: 0  t_599: 1\n",
      "x_600: 1  t_600: 0\n",
      "x_601: 1  t_601: 1\n",
      "x_602: 0  t_602: 0\n",
      "x_603: 1  t_603: 0\n",
      "x_604: 1  t_604: 1\n",
      "x_605: 0  t_605: 0\n",
      "x_606: 1  t_606: 1\n",
      "x_607: 1  t_607: 1\n",
      "x_608: 0  t_608: 0\n",
      "x_609: 0  t_609: 1\n",
      "x_610: 0  t_610: 0\n",
      "x_611: 0  t_611: 0\n",
      "x_612: 0  t_612: 0\n",
      "x_613: 1  t_613: 1\n",
      "x_614: 0  t_614: 0\n",
      "x_615: 0  t_615: 0\n",
      "x_616: 0  t_616: 0\n",
      "x_617: 0  t_617: 0\n",
      "x_618: 0  t_618: 1\n",
      "x_619: 0  t_619: 0\n",
      "x_620: 0  t_620: 0\n",
      "x_621: 1  t_621: 1\n",
      "x_622: 0  t_622: 0\n",
      "x_623: 0  t_623: 1\n",
      "x_624: 0  t_624: 1\n",
      "x_625: 1  t_625: 1\n",
      "x_626: 0  t_626: 1\n",
      "x_627: 1  t_627: 1\n",
      "x_628: 1  t_628: 0\n",
      "x_629: 0  t_629: 0\n",
      "x_630: 0  t_630: 0\n",
      "x_631: 1  t_631: 1\n",
      "x_632: 1  t_632: 1\n",
      "x_633: 0  t_633: 1\n",
      "x_634: 0  t_634: 0\n",
      "x_635: 0  t_635: 1\n",
      "x_636: 0  t_636: 0\n",
      "x_637: 0  t_637: 1\n",
      "x_638: 0  t_638: 0\n",
      "x_639: 1  t_639: 0\n",
      "x_640: 0  t_640: 1\n",
      "x_641: 0  t_641: 1\n",
      "x_642: 0  t_642: 1\n",
      "x_643: 0  t_643: 0\n",
      "x_644: 0  t_644: 1\n",
      "x_645: 0  t_645: 0\n",
      "x_646: 0  t_646: 1\n",
      "x_647: 0  t_647: 1\n",
      "x_648: 1  t_648: 0\n",
      "x_649: 1  t_649: 1\n",
      "x_650: 0  t_650: 0\n",
      "x_651: 1  t_651: 1\n",
      "x_652: 1  t_652: 1\n",
      "x_653: 0  t_653: 1\n",
      "x_654: 0  t_654: 0\n",
      "x_655: 0  t_655: 1\n",
      "x_656: 0  t_656: 0\n",
      "x_657: 1  t_657: 0\n",
      "x_658: 1  t_658: 1\n",
      "x_659: 1  t_659: 1\n",
      "x_660: 0  t_660: 0\n",
      "x_661: 0  t_661: 0\n",
      "x_662: 0  t_662: 0\n",
      "x_663: 0  t_663: 1\n",
      "x_664: 1  t_664: 1\n",
      "x_665: 0  t_665: 1\n",
      "x_666: 0  t_666: 1\n",
      "x_667: 1  t_667: 0\n",
      "x_668: 0  t_668: 0\n",
      "x_669: 0  t_669: 1\n",
      "x_670: 1  t_670: 0\n",
      "x_671: 0  t_671: 1\n",
      "x_672: 0  t_672: 1\n",
      "x_673: 0  t_673: 0\n",
      "x_674: 1  t_674: 0\n",
      "x_675: 0  t_675: 0\n",
      "x_676: 0  t_676: 1\n",
      "x_677: 0  t_677: 0\n",
      "x_678: 1  t_678: 0\n",
      "x_679: 1  t_679: 1\n",
      "x_680: 0  t_680: 1\n",
      "x_681: 0  t_681: 0\n",
      "x_682: 0  t_682: 1\n",
      "x_683: 0  t_683: 0\n",
      "x_684: 1  t_684: 0\n",
      "x_685: 0  t_685: 0\n",
      "x_686: 0  t_686: 0\n",
      "x_687: 0  t_687: 0\n",
      "x_688: 0  t_688: 1\n",
      "x_689: 0  t_689: 0\n",
      "x_690: 0  t_690: 1\n",
      "x_691: 0  t_691: 1\n",
      "x_692: 0  t_692: 1\n",
      "x_693: 0  t_693: 1\n",
      "x_694: 0  t_694: 1\n",
      "x_695: 0  t_695: 1\n",
      "x_696: 1  t_696: 1\n",
      "x_697: 0  t_697: 0\n",
      "x_698: 0  t_698: 0\n",
      "x_699: 0  t_699: 0\n",
      "x_700: 0  t_700: 1\n",
      "x_701: 0  t_701: 1\n",
      "x_702: 0  t_702: 0\n",
      "x_703: 1  t_703: 1\n",
      "x_704: 0  t_704: 1\n",
      "x_705: 0  t_705: 1\n",
      "x_706: 0  t_706: 0\n",
      "x_707: 0  t_707: 0\n",
      "x_708: 0  t_708: 1\n",
      "x_709: 1  t_709: 1\n",
      "x_710: 0  t_710: 0\n",
      "x_711: 0  t_711: 0\n",
      "x_712: 0  t_712: 0\n",
      "x_713: 1  t_713: 0\n",
      "x_714: 1  t_714: 1\n",
      "x_715: 1  t_715: 1\n",
      "x_716: 0  t_716: 1\n",
      "x_717: 0  t_717: 1\n",
      "x_718: 1  t_718: 1\n",
      "x_719: 0  t_719: 1\n",
      "x_720: 0  t_720: 0\n",
      "x_721: 0  t_721: 0\n",
      "x_722: 0  t_722: 1\n",
      "x_723: 0  t_723: 0\n",
      "x_724: 0  t_724: 1\n",
      "x_725: 1  t_725: 1\n",
      "x_726: 1  t_726: 1\n",
      "x_727: 0  t_727: 0\n",
      "x_728: 0  t_728: 1\n",
      "x_729: 0  t_729: 0\n",
      "x_730: 0  t_730: 1\n",
      "x_731: 1  t_731: 1\n",
      "x_732: 0  t_732: 1\n",
      "x_733: 0  t_733: 1\n",
      "x_734: 1  t_734: 1\n",
      "x_735: 0  t_735: 0\n",
      "x_736: 1  t_736: 1\n",
      "x_737: 0  t_737: 0\n",
      "x_738: 1  t_738: 0\n",
      "x_739: 1  t_739: 1\n",
      "x_740: 0  t_740: 1\n",
      "x_741: 0  t_741: 0\n",
      "x_742: 0  t_742: 1\n",
      "x_743: 0  t_743: 1\n",
      "x_744: 0  t_744: 1\n",
      "x_745: 0  t_745: 0\n",
      "x_746: 1  t_746: 1\n",
      "x_747: 0  t_747: 1\n",
      "x_748: 0  t_748: 0\n",
      "x_749: 1  t_749: 1\n",
      "x_750: 0  t_750: 1\n",
      "x_751: 0  t_751: 1\n",
      "x_752: 0  t_752: 0\n",
      "x_753: 0  t_753: 0\n",
      "x_754: 0  t_754: 1\n",
      "x_755: 0  t_755: 1\n",
      "x_756: 0  t_756: 0\n",
      "x_757: 0  t_757: 0\n",
      "x_758: 1  t_758: 0\n",
      "x_759: 0  t_759: 1\n",
      "x_760: 0  t_760: 0\n",
      "x_761: 1  t_761: 0\n",
      "x_762: 0  t_762: 1\n",
      "x_763: 1  t_763: 1\n",
      "x_764: 1  t_764: 0\n",
      "x_765: 0  t_765: 0\n",
      "x_766: 1  t_766: 1\n",
      "x_767: 0  t_767: 0\n",
      "x_768: 0  t_768: 1\n",
      "x_769: 0  t_769: 0\n",
      "x_770: 0  t_770: 0\n",
      "x_771: 1  t_771: 0\n",
      "x_772: 0  t_772: 1\n",
      "x_773: 0  t_773: 1\n",
      "x_774: 1  t_774: 1\n",
      "x_775: 1  t_775: 0\n",
      "x_776: 1  t_776: 0\n",
      "x_777: 0  t_777: 1\n",
      "x_778: 0  t_778: 1\n",
      "x_779: 0  t_779: 0\n",
      "x_780: 1  t_780: 1\n",
      "x_781: 0  t_781: 1\n",
      "x_782: 0  t_782: 1\n",
      "x_783: 0  t_783: 1\n",
      "x_784: 0  t_784: 1\n",
      "x_785: 0  t_785: 0\n",
      "x_786: 1  t_786: 1\n",
      "x_787: 0  t_787: 0\n",
      "x_788: 0  t_788: 0\n",
      "x_789: 0  t_789: 0\n",
      "x_790: 0  t_790: 0\n",
      "x_791: 0  t_791: 0\n",
      "x_792: 1  t_792: 0\n",
      "x_793: 0  t_793: 0\n",
      "x_794: 0  t_794: 1\n",
      "x_795: 0  t_795: 1\n",
      "x_796: 1  t_796: 1\n",
      "x_797: 0  t_797: 0\n",
      "x_798: 0  t_798: 0\n",
      "x_799: 0  t_799: 1\n",
      "x_800: 0  t_800: 0\n",
      "x_801: 0  t_801: 0\n",
      "x_802: 0  t_802: 0\n",
      "x_803: 0  t_803: 1\n",
      "x_804: 0  t_804: 0\n",
      "x_805: 0  t_805: 1\n",
      "x_806: 0  t_806: 0\n",
      "x_807: 0  t_807: 0\n",
      "x_808: 0  t_808: 1\n",
      "x_809: 0  t_809: 1\n",
      "x_810: 0  t_810: 0\n",
      "x_811: 0  t_811: 0\n",
      "x_812: 0  t_812: 0\n",
      "x_813: 0  t_813: 0\n",
      "x_814: 0  t_814: 0\n",
      "x_815: 0  t_815: 1\n",
      "x_816: 0  t_816: 1\n",
      "x_817: 1  t_817: 1\n",
      "x_818: 0  t_818: 0\n",
      "x_819: 1  t_819: 0\n",
      "x_820: 1  t_820: 1\n",
      "x_821: 0  t_821: 1\n",
      "x_822: 0  t_822: 1\n",
      "x_823: 0  t_823: 1\n",
      "x_824: 0  t_824: 0\n",
      "x_825: 0  t_825: 1\n",
      "x_826: 1  t_826: 1\n",
      "x_827: 1  t_827: 0\n",
      "x_828: 1  t_828: 0\n",
      "x_829: 0  t_829: 0\n",
      "x_830: 0  t_830: 1\n",
      "x_831: 0  t_831: 0\n",
      "x_832: 1  t_832: 1\n",
      "x_833: 0  t_833: 1\n",
      "x_834: 0  t_834: 0\n",
      "x_835: 1  t_835: 0\n",
      "x_836: 1  t_836: 0\n",
      "x_837: 0  t_837: 1\n",
      "x_838: 0  t_838: 0\n",
      "x_839: 0  t_839: 0\n",
      "x_840: 1  t_840: 1\n",
      "x_841: 1  t_841: 0\n",
      "x_842: 0  t_842: 1\n",
      "x_843: 1  t_843: 1\n",
      "x_844: 1  t_844: 0\n",
      "x_845: 0  t_845: 1\n",
      "x_846: 0  t_846: 1\n",
      "x_847: 0  t_847: 1\n",
      "x_848: 0  t_848: 1\n",
      "x_849: 1  t_849: 0\n",
      "x_850: 1  t_850: 1\n",
      "x_851: 0  t_851: 0\n",
      "x_852: 0  t_852: 0\n",
      "x_853: 0  t_853: 0\n",
      "x_854: 0  t_854: 0\n",
      "x_855: 1  t_855: 1\n",
      "x_856: 0  t_856: 0\n",
      "x_857: 0  t_857: 1\n",
      "x_858: 0  t_858: 1\n",
      "x_859: 0  t_859: 0\n",
      "x_860: 0  t_860: 1\n",
      "x_861: 0  t_861: 1\n",
      "x_862: 1  t_862: 1\n",
      "x_863: 0  t_863: 1\n",
      "x_864: 0  t_864: 0\n",
      "x_865: 0  t_865: 1\n",
      "x_866: 1  t_866: 0\n",
      "x_867: 1  t_867: 1\n",
      "x_868: 0  t_868: 1\n",
      "x_869: 1  t_869: 1\n",
      "x_870: 0  t_870: 0\n",
      "x_871: 0  t_871: 1\n",
      "x_872: 0  t_872: 1\n",
      "x_873: 0  t_873: 0\n",
      "x_874: 0  t_874: 1\n",
      "x_875: 0  t_875: 0\n",
      "x_876: 0  t_876: 1\n",
      "x_877: 0  t_877: 0\n",
      "x_878: 0  t_878: 1\n",
      "x_879: 0  t_879: 0\n",
      "x_880: 0  t_880: 1\n",
      "x_881: 1  t_881: 0\n",
      "x_882: 0  t_882: 1\n",
      "x_883: 0  t_883: 0\n",
      "x_884: 0  t_884: 0\n",
      "x_885: 0  t_885: 0\n",
      "x_886: 0  t_886: 0\n",
      "x_887: 1  t_887: 1\n",
      "x_888: 0  t_888: 1\n",
      "x_889: 0  t_889: 0\n",
      "x_890: 0  t_890: 0\n",
      "x_891: 0  t_891: 1\n",
      "x_892: 0  t_892: 0\n",
      "x_893: 1  t_893: 1\n",
      "x_894: 1  t_894: 1\n",
      "x_895: 0  t_895: 0\n",
      "x_896: 1  t_896: 1\n",
      "x_897: 1  t_897: 0\n",
      "x_898: 0  t_898: 1\n",
      "x_899: 0  t_899: 1\n",
      "x_900: 0  t_900: 1\n",
      "x_901: 0  t_901: 1\n",
      "x_902: 1  t_902: 0\n",
      "x_903: 0  t_903: 0\n",
      "x_904: 0  t_904: 0\n",
      "x_905: 0  t_905: 1\n",
      "x_906: 1  t_906: 0\n",
      "x_907: 1  t_907: 1\n",
      "x_908: 0  t_908: 0\n",
      "x_909: 0  t_909: 0\n",
      "x_910: 0  t_910: 0\n",
      "x_911: 0  t_911: 1\n",
      "x_912: 1  t_912: 0\n",
      "x_913: 1  t_913: 1\n",
      "x_914: 0  t_914: 1\n",
      "x_915: 1  t_915: 1\n",
      "x_916: 0  t_916: 0\n",
      "x_917: 0  t_917: 0\n",
      "x_918: 0  t_918: 1\n",
      "x_919: 0  t_919: 0\n",
      "x_920: 0  t_920: 1\n",
      "x_921: 0  t_921: 0\n",
      "x_922: 0  t_922: 0\n",
      "x_923: 0  t_923: 0\n",
      "x_924: 0  t_924: 0\n",
      "x_925: 0  t_925: 1\n",
      "x_926: 1  t_926: 0\n",
      "x_927: 0  t_927: 0\n",
      "x_928: 0  t_928: 0\n",
      "x_929: 0  t_929: 0\n",
      "x_930: 0  t_930: 0\n",
      "x_931: 0  t_931: 0\n",
      "x_932: 0  t_932: 1\n",
      "x_933: 1  t_933: 0\n",
      "x_934: 0  t_934: 1\n",
      "x_935: 0  t_935: 1\n",
      "x_936: 0  t_936: 1\n",
      "x_937: 0  t_937: 1\n",
      "x_938: 0  t_938: 1\n",
      "x_939: 0  t_939: 1\n",
      "x_940: 1  t_940: 1\n",
      "x_941: 0  t_941: 1\n",
      "x_942: 1  t_942: 1\n",
      "x_943: 0  t_943: 0\n",
      "x_944: 0  t_944: 0\n",
      "x_945: 0  t_945: 0\n",
      "x_946: 0  t_946: 1\n",
      "x_947: 1  t_947: 0\n",
      "x_948: 0  t_948: 1\n",
      "x_949: 0  t_949: 0\n",
      "x_950: 0  t_950: 1\n",
      "x_951: 0  t_951: 0\n",
      "x_952: 0  t_952: 0\n",
      "x_953: 0  t_953: 0\n",
      "x_954: 0  t_954: 1\n",
      "x_955: 0  t_955: 1\n",
      "x_956: 0  t_956: 1\n",
      "x_957: 0  t_957: 0\n",
      "x_958: 0  t_958: 1\n",
      "x_959: 0  t_959: 0\n",
      "x_960: 0  t_960: 0\n",
      "x_961: 0  t_961: 0\n",
      "x_962: 0  t_962: 1\n",
      "x_963: 1  t_963: 1\n",
      "x_964: 0  t_964: 0\n",
      "x_965: 0  t_965: 1\n",
      "x_966: 1  t_966: 1\n",
      "x_967: 0  t_967: 0\n",
      "x_968: 0  t_968: 0\n",
      "x_969: 0  t_969: 0\n",
      "x_970: 0  t_970: 0\n",
      "x_971: 0  t_971: 1\n",
      "x_972: 0  t_972: 0\n",
      "x_973: 1  t_973: 0\n",
      "x_974: 0  t_974: 0\n",
      "x_975: 0  t_975: 0\n",
      "x_976: 0  t_976: 0\n",
      "x_977: 1  t_977: 1\n",
      "x_978: 0  t_978: 0\n",
      "x_979: 1  t_979: 0\n",
      "x_980: 0  t_980: 0\n",
      "x_981: 0  t_981: 0\n",
      "x_982: 0  t_982: 0\n",
      "x_983: 0  t_983: 0\n",
      "x_984: 0  t_984: 0\n",
      "x_985: 0  t_985: 1\n",
      "x_986: 0  t_986: 1\n",
      "x_987: 0  t_987: 1\n",
      "x_988: 0  t_988: 1\n",
      "x_989: 1  t_989: 1\n",
      "x_990: 0  t_990: 1\n",
      "x_991: 0  t_991: 1\n",
      "x_992: 0  t_992: 0\n",
      "x_993: 0  t_993: 0\n",
      "x_994: 0  t_994: 1\n",
      "x_995: 1  t_995: 1\n",
      "x_996: 1  t_996: 0\n",
      "x_997: 1  t_997: 1\n",
      "x_998: 0  t_998: 1\n",
      "x_999: 0  t_999: 1\n",
      "\n",
      "Let's evaluate the predictors on average training loss.\n",
      "y_1 average training loss: 0.42\n",
      "y_2 average training loss: 0.58\n",
      "y_3 average training loss: 0.47\n",
      "y_4 average training loss: 0.53\n",
      "\n",
      "Which is an optimal predictor in terms of average training loss? y_1\n",
      "What expected loss does it get? 0.40\n"
     ]
    }
   ],
   "source": [
    "N = 1000\n",
    "train_set = sample_data_set(N)\n",
    "print(\"Training set\")\n",
    "for i, (x, t) in enumerate(train_set):\n",
    "  print(f\"x_{i}: {x}  t_{i}: {t}\")\n",
    "\n",
    "print(\"\\nLet's evaluate the predictors on average training loss.\")\n",
    "predictor_losses = [average_loss(y, train_set) for y in all_predictors]\n",
    "for i, predictor_loss in enumerate(predictor_losses):\n",
    "  print(f\"y_{i+1} average training loss: \" + \"{:.2f}\".format(predictor_loss))\n",
    "\n",
    "# This code finds the predictor with the minimum training loss\n",
    "which_predictor = np.argmin(predictor_losses)\n",
    "best_predictor = all_predictors[which_predictor]\n",
    "\n",
    "# Report\n",
    "print(f\"\\nWhich is an optimal predictor in terms of average training loss? y_{which_predictor+1}\")\n",
    "print(\"What expected loss does it get? {:.2f}\".format(expected_loss(best_predictor, data_gen_joint)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJr15nS8PflB"
   },
   "source": [
    "If you run the previous cell a few times, you'll see that the best predictor $\\hat{y}^{\\star}$ is *random*, because our training set was *random*. Let's automate this process and see *how often* we pick the optimal predictor $\\hat{y}^{\\star} = y_1$ from random training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "XHjY1Ff-IUT-"
   },
   "outputs": [],
   "source": [
    "def pick_predictor(train_set):\n",
    "  \"\"\"Pick the best predictor on this train set.\"\"\"\n",
    "  predictor_losses = [average_loss(y, train_set) for y in all_predictors]\n",
    "  i = np.argmin(predictor_losses)\n",
    "  return all_predictors[i], i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5R__iH7jQs-R"
   },
   "source": [
    "With the code below you can vary the size of training sets $N$ and the number of random training sets that we sample $m$ to estimate the probability of picking the optimal predictor. Notice, as $N \\to \\infty$, we end up always picking $y_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tZNADlrcIlOw",
    "outputId": "5c8fcc63-9f06-4acc-b6b8-dd369ae1410b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best predictor on train set 0 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 1 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 2 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 3 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 4 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 5 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 6 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 7 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 8 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 9 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 10 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 11 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 12 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 13 is y_3 and it has expected loss R[y_3]=0.45\n",
      "Best predictor on train set 14 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 15 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 16 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 17 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 18 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 19 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 20 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 21 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 22 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 23 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 24 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 25 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 26 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 27 is y_3 and it has expected loss R[y_3]=0.45\n",
      "Best predictor on train set 28 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 29 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 30 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 31 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 32 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 33 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 34 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 35 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 36 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 37 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 38 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 39 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 40 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 41 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 42 is y_3 and it has expected loss R[y_3]=0.45\n",
      "Best predictor on train set 43 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 44 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 45 is y_3 and it has expected loss R[y_3]=0.45\n",
      "Best predictor on train set 46 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 47 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 48 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 49 is y_3 and it has expected loss R[y_3]=0.45\n",
      "Best predictor on train set 50 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 51 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 52 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 53 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 54 is y_3 and it has expected loss R[y_3]=0.45\n",
      "Best predictor on train set 55 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 56 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 57 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 58 is y_3 and it has expected loss R[y_3]=0.45\n",
      "Best predictor on train set 59 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 60 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 61 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 62 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 63 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 64 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 65 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 66 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 67 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 68 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 69 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 70 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 71 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 72 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 73 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 74 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 75 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 76 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 77 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 78 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 79 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 80 is y_3 and it has expected loss R[y_3]=0.45\n",
      "Best predictor on train set 81 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 82 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 83 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 84 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 85 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 86 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 87 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 88 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 89 is y_3 and it has expected loss R[y_3]=0.45\n",
      "Best predictor on train set 90 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 91 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 92 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 93 is y_3 and it has expected loss R[y_3]=0.45\n",
      "Best predictor on train set 94 is y_3 and it has expected loss R[y_3]=0.45\n",
      "Best predictor on train set 95 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 96 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 97 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 98 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 99 is y_1 and it has expected loss R[y_1]=0.40\n",
      "\n",
      "Over 100 training sets of size 100, we picked the optimal predictor y_1 89.0 percent of the time\n"
     ]
    }
   ],
   "source": [
    "N = 100\n",
    "m = 100\n",
    "\n",
    "how_often_optimal = 0\n",
    "for j in range(m):\n",
    "  # Sampling a new training set of size N\n",
    "  train_set = sample_data_set(N)\n",
    "\n",
    "  # Pick the best predictor\n",
    "  predictor, which_predictor = pick_predictor(train_set)\n",
    "  how_often_optimal += (which_predictor + 1 == 1)\n",
    "    \n",
    "  # Report\n",
    "  print((f\"Best predictor on train set {j} is \" +\n",
    "        f\"y_{which_predictor+1} and it has expected loss R[y_{which_predictor+1}]=\" + \n",
    "        \"{:.2f}\".format(expected_loss(predictor, data_gen_joint))))\n",
    "\n",
    "# Report\n",
    "print((f\"\\nOver {m} training sets of size {N}, \"+\n",
    "       f\"we picked the optimal predictor y_1 {how_often_optimal/m * 100} percent of the time\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's see how good of an approximation we can form of the expected loss $\\hat{\\mathcal{R}}[\\hat{y}^{\\star}, \\mathcal{D}^{test}] \\approx \\mathcal{R}[\\hat{y}^{\\star}]$ using test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best training set predictor y_2 has expected loss 0.60 and average test loss 0.61 on test set of size 1000 and average training loss 0.00 on test set of size 1.\n"
     ]
    }
   ],
   "source": [
    "train_N = 1\n",
    "test_N = 1000\n",
    "test_set = sample_data_set(test_N)\n",
    "\n",
    "# Sampling a new training set of size N\n",
    "train_set = sample_data_set(train_N)\n",
    "\n",
    "# Pick the best predictor\n",
    "predictor, which_predictor = pick_predictor(train_set)\n",
    "\n",
    "# Report\n",
    "print((f\"Best training set predictor y_{which_predictor+1} \"+\n",
    "       \"has expected loss {:.2f} and \".format(expected_loss(predictor, data_gen_joint)) +\n",
    "       \"average test loss {:.2f} on test set of size {} and \".format(average_loss(predictor, test_set), test_N) +\n",
    "       \"average training loss {:.2f} on test set of size {}.\".format(average_loss(predictor, train_set), train_N)\n",
    "      ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "lec03.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
